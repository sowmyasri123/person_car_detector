{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SowmyaSri\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: IEPlugin class is deprecated. Please use IECore class instead.\n",
      "C:\\Users\\SowmyaSri\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: Reading network using constructor is deprecated. Please, use IECore.read_network() method instead\n"
     ]
    }
   ],
   "source": [
    "import sys, os, cv2, time\n",
    "import numpy as np, math\n",
    "from argparse import ArgumentParser\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "m_input_size = 416\n",
    "\n",
    "yolo_scale_13 = 13\n",
    "yolo_scale_26 = 26\n",
    "yolo_scale_52 = 52\n",
    "\n",
    "classes = 3\n",
    "coords = 4\n",
    "num = 6\n",
    "anchors = [10,14, 23,27, 37,58, 81,82, 135,169, 344,319]\n",
    "\n",
    "LABELS = (\"car\",\"person\")\n",
    "\n",
    "label_text_color = (255, 0,0)\n",
    "label_background_color = (125, 175, 75)\n",
    "box_color = (0, 128, 0)\n",
    "box_thickness = 2\n",
    "\n",
    "model_xml = \"C:/Users/SowmyaSri/Desktop/person/frozen_darknet_yolov3_model.xml\" #<--- MYRIAD\n",
    "model_bin = \"C:/Users/SowmyaSri/Desktop/person/frozen_darknet_yolov3_model.bin\"\n",
    "plugin = IEPlugin(device=\"GPU\")\n",
    "net = IENetwork(model=model_xml, weights=model_bin)\n",
    "input_blob = next(iter(net.inputs))\n",
    "exec_net = plugin.load(network=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_argparser():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-d\", \"--device\", help=\"Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. \\\n",
    "                                                Sample will look for a suitable plugin for device specified (CPU by default)\", default=\"CPU\", type=str)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def EntryIndex(side, lcoords, lclasses, location, entry):\n",
    "    n = int(location / (side * side))\n",
    "    loc = location % (side * side)\n",
    "    return int(n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc)\n",
    "\n",
    "\n",
    "class DetectionObject():\n",
    "    xmin = 0\n",
    "    ymin = 0\n",
    "    xmax = 0\n",
    "    ymax = 0\n",
    "    class_id = 0\n",
    "    confidence = 0.0\n",
    "\n",
    "    def __init__(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n",
    "        self.xmin = int((x - w / 2) * w_scale)\n",
    "        self.ymin = int((y - h / 2) * h_scale)\n",
    "        self.xmax = int(self.xmin + w * w_scale)\n",
    "        self.ymax = int(self.ymin + h * h_scale)\n",
    "        self.class_id = class_id\n",
    "        self.confidence = confidence\n",
    "\n",
    "\n",
    "def IntersectionOverUnion(box_1, box_2):\n",
    "    width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)\n",
    "    height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)\n",
    "    area_of_overlap = 0.0\n",
    "    if (width_of_overlap_area < 0.0 or height_of_overlap_area < 0.0):\n",
    "        area_of_overlap = 0.0\n",
    "    else:\n",
    "        area_of_overlap = width_of_overlap_area * height_of_overlap_area\n",
    "    box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin)\n",
    "    box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin)\n",
    "    area_of_union = box_1_area + box_2_area - area_of_overlap\n",
    "    retval = 0.0\n",
    "    if area_of_union <= 0.0:\n",
    "        retval = 0.0\n",
    "    else:\n",
    "        retval = (area_of_overlap / area_of_union)\n",
    "    return retval\n",
    "def ParseYOLOV3Output(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects):\n",
    "\n",
    "    out_blob_h = blob.shape[2]\n",
    "    out_blob_w = blob.shape[3]\n",
    "\n",
    "    side = out_blob_h\n",
    "    #print(\"side\",out_blob_h)\n",
    "    anchor_offset = 0\n",
    "\n",
    "    if len(anchors) == 18:   ## YoloV3\n",
    "        if side == yolo_scale_13:\n",
    "            anchor_offset = 2 * 6\n",
    "        elif side == yolo_scale_26:\n",
    "            anchor_offset = 2 * 3\n",
    "        elif side == yolo_scale_52:\n",
    "            anchor_offset = 2 * 0\n",
    "\n",
    "    elif len(anchors) == 12: ## tiny-YoloV3\n",
    "        #print(\"yes length is 12\")\n",
    "        if side == yolo_scale_13:\n",
    "            anchor_offset = 2 * 3\n",
    "        elif side == 26:\n",
    "            anchor_offset = 2 * 0\n",
    "            #print(\"anchor_offset\",anchor_offset)\n",
    "\n",
    "    else:                    ## ???\n",
    "        if side == yolo_scale_13:\n",
    "            anchor_offset = 2 * 6\n",
    "        elif side == yolo_scale_26:\n",
    "            anchor_offset = 2 * 3\n",
    "        elif side == yolo_scale_52:\n",
    "            anchor_offset = 2 * 0\n",
    "\n",
    "    side_square = side * side\n",
    "    output_blob = blob.flatten()\n",
    "\n",
    "    for i in range(side_square):\n",
    "        row = int(i / side)\n",
    "        col = int(i % side)\n",
    "        for n in range(3):\n",
    "            obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords)\n",
    "            #print(obj_index)\n",
    "            box_index = EntryIndex(side, coords, classes, n * side * side + i, 0)\n",
    "            scale = output_blob[obj_index]\n",
    "            if (scale < threshold):\n",
    "                continue\n",
    "            x = (col + output_blob[box_index + 0 * side_square]) / side * resized_im_w\n",
    "            y = (row + output_blob[box_index + 1 * side_square]) / side * resized_im_h\n",
    "            height = math.exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1]\n",
    "            width = math.exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n]\n",
    "            for j in range(classes):\n",
    "                class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j)\n",
    "                prob = scale * output_blob[class_index]\n",
    "                if prob < threshold:\n",
    "                    continue\n",
    "                obj = DetectionObject(x, y, height, width, j, prob, (original_im_h / resized_im_h), (original_im_w / resized_im_w))\n",
    "                objects.append(obj)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videosFrameCount = 835\n",
      "videosFPS = 29\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 14196 is out of bounds for axis 0 with size 14196",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1f172e4f2f98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0melapsedTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mfps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"(Playback) {:.1f} FPS\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0melapsedTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mmain_IE_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-1f172e4f2f98>\u001b[0m in \u001b[0;36mmain_IE_infer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mobjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParseYOLOV3Output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Filtering overlapping boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4ca47637ec5d>\u001b[0m in \u001b[0;36mParseYOLOV3Output\u001b[1;34m(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[0mclass_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntryIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mside_square\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutput_blob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 14196 is out of bounds for axis 0 with size 14196"
     ]
    }
   ],
   "source": [
    "def main_IE_infer():\n",
    "    fps = \"\"\n",
    "    framepos = 0\n",
    "    frame_count = 0\n",
    "    vidfps = 0\n",
    "    skip_frame = 0\n",
    "    elapsedTime = 0\n",
    "    cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/t1.MOV\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/customer_test.avi\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/hp4.mp4\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/mgc22.avi\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/Recording.avi\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/family_6.webm\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/mgc1.mp4\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/age_test11.webm\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/test_videos/mgc2.mp4\")\n",
    "#     cap = cv2.VideoCapture(\"C:/Users/SowmyaSri/Downloads/hg1.mp4\")\n",
    "\n",
    "    camera_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    camera_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    vidfps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"videosFrameCount =\", str(frame_count))\n",
    "    print(\"videosFPS =\", str(vidfps))\n",
    "    new_w = int(camera_width * m_input_size/camera_width)\n",
    "    new_h = int(camera_height * m_input_size/camera_height)\n",
    "    time.sleep(1)\n",
    "    while cap.isOpened():\n",
    "        t1 = time.time()\n",
    "        ## Uncomment only when playing video files\n",
    "        #cap.set(cv2.CAP_PROP_POS_FRAMES, framepos)\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "#         resized_image = cv2.resize(image, (new_w, new_h), interpolation = cv2.INTER_CUBIC)\n",
    "#         canvas = np.full((m_input_size, m_input_size, 3), 128)\n",
    "#         canvas[(m_input_size-new_h)//2:(m_input_size-new_h)//2 + new_h,(m_input_size-new_w)//2:(m_input_size-new_w)//2 + new_w,  :] = resized_image\n",
    "#         prepimg = canvas\n",
    "#         prepimg = prepimg[np.newaxis, :, :, :]     # Batch size axis add\n",
    "#         prepimg = prepimg.transpose((0, 3, 1, 2))  # NHWC to NCHW\n",
    "        \n",
    "        \n",
    "        prepimg = cv2.resize(image, (new_w,new_h))\n",
    "        prepimg = prepimg.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        prepimg = prepimg.reshape((1, 3,416,416))\n",
    "        outputs = exec_net.infer(inputs={input_blob: prepimg})\n",
    "\n",
    "        #output_name = detector/yolo-v3-tiny/Conv_12/BiasAdd/YoloRegion\n",
    "        #output_name = detector/yolo-v3-tiny/Conv_9/BiasAdd/YoloRegion\n",
    "\n",
    "        objects = []\n",
    "\n",
    "        for output in outputs.values():\n",
    "            objects = ParseYOLOV3Output(output, new_h, new_w, camera_height, camera_width, 0.4, objects)\n",
    "\n",
    "        # Filtering overlapping boxes\n",
    "        objlen = len(objects)\n",
    "        for i in range(objlen):\n",
    "            if (objects[i].confidence == 0.0):\n",
    "                continue\n",
    "            for j in range(i + 1, objlen):\n",
    "                if (IntersectionOverUnion(objects[i], objects[j]) >= 0.9):\n",
    "                    if objects[i].confidence < objects[j].confidence:\n",
    "                        objects[i], objects[j] = objects[j], objects[i]\n",
    "                    objects[j].confidence = 0.0\n",
    "        \n",
    "        # Drawing boxes\n",
    "        for obj in objects:\n",
    "            objconf=obj.confidence*100\n",
    "            if objconf < 57:\n",
    "                continue\n",
    "            label = obj.class_id\n",
    "            confidence = obj.confidence\n",
    "            label_text = LABELS[label] + \" (\" + \"{:.1f}\".format(confidence * 100) + \"%)\"\n",
    "            cv2.rectangle(image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), (0, 255, 0), box_thickness)\n",
    "            cv2.putText(image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, label_text_color, 2)\n",
    "            print(\"conf\",label_text,confidence)\n",
    "        cv2.putText(image, fps, (camera_width - 170, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        frame = cv2.resize(image, (720,720 )) \n",
    "        cv2.imshow(\"Result\", frame)\n",
    "\n",
    "        if cv2.waitKey(1)&0xFF == ord('q'):\n",
    "            break\n",
    "        elapsedTime = time.time() - t1\n",
    "        fps = \"(Playback) {:.1f} FPS\".format(1/elapsedTime)\n",
    "main_IE_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.002645\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "time.sleep(1)\n",
    "returned_time = datetime.now()\n",
    "print(returned_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "t1 = datetime.now()\n",
    "time.sleep(2)\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(delta.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
